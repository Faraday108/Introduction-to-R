---
title: "R Syntax Examples"
author: "Nathan Young"
output: pdf_document
---

# R Nuts and Bolts
## Entering Input
We type expressions into the R prompt. 
The <- symbol is the assignment operator
```{r}
x <- 5 # Assignment of value to variable
print(x) # Explicit printing
x # auto-printing
y <- 10:20
```
When [1] is printed it indicates that x is a vector and that 5 is the first element
The : operator is used to create integer sequences

## R Objects
R has five basic classes of objects
- Character
- Numeric (real numbers)
- Integer
- Complex
- Logical (True/False)

The vector is the most basic object type, initialized with vector() function.  
A vector can only contain objects of the same class
A *list* is represented as a vector but can contain objects of different classes. 

## Numbers
Numbers are generally treated as numeric objects (such as double precision real numbers). Meaning, even if you see a '1' it is represented behind the scene as a numeric object like '1.00'. 
If you explicitly want an integer, use the L suffix. Entering '1' gives you a numeric object, while entering '1L' gives an integer object.   
Inf represents infinity. 
NaN represents an undefined value such as 0/0

## Attributes
R Objects can have many attributes (like metadata) to help describe the object. 
* Names, dimnames
* dimensions (matrices, arrays)
* class (integer, numeric)
* length
* other user-defined attributes/metadata

Attributes can be accessed using attributes() function. 

## Creating Vectors
The c() function can be used to create vectors of objects by concatenating things together. 
```{r}
x <- c(0.5, 0.6)       # numeric
x <- c(TRUE, FALSE)    # logical
x <- c(T, F)           # logical
x <- c("a", "b", "c")  # character
x <- 9:29              # integer
x <- c(1+0i, 2+4i)     # complex
x <- vector("numeric", length = 10) # initialize vector
```

## Mixing Objects
Sometimes we mix objects (on accident or on purpose).  
When different objects are mixed in a vector, they are *coerced* to be the same class. Sometimes this works the way you expect others, not. 

## Explicit Coercion
You can explicitly coerce between classes using the as.* function
```{r}
x <- 0:6
class(x)
as.numeric(x)
as.logical(x)
as.character(x)
```
Sometimes, R doesn't know what to do and will return NA. 

## Matrices 
Vectors with a *dimension* attribute. The dimension attribute is an integer vector of length 2
```{r}
m <- matrix (nrow = 2, ncol = 3)
dim(m)
attributes(m)
```
Matrics are constructed *column-wise*, so entries can be thought of starting in the "upper left" corner and running down the columns. 
```{r}
m <- matrix(1:6,nrow = 2, ncol = 3)
m
```
Can also be created directly from vectors by adding a dimension attribute. 
```{r}
m <- 1:10
m
dim(m) <- c(2,5)
m
```
Can be created with *column-binding* or *row-binding*
```{r}
x <- 1:3
y <- 10:13
cbind(x,y)
rbind(x,y)
```

## Lists
Special vectors that contain different elements of different classes.  
Can be explicitly created with the list() function. 
```{r}
x <- list(1, "a", TRUE, 1+4i)
x
```
Can also create an empty list of length with the vector() function.  
```{r}
x <- vector("list", length = 5)
```

## Factors
Used to represent categorical data that is unordered or ordered. Akin to integer vector where each integer has a label. Important for statical modeling and treated special in functions like lm() and glm().  
Using factors with labels is better than using integers because factors are self-describing. 
Factor with values "Male" and "Female" is better than variable with values 1 and 2. 
Create with factor() function
```{r}
x <- factor(c("yes", "yes", "no", "yes", "yes"))
x
table(x)
unclass(x)
```
Often, factors are automatically created when reading data with function like data.table().  
Order of levels can be set using levels argument to factor(). Often important in linear modeling with first level as baseline. 

## Missing Values
Denoted by NA or NaN for undefined mathematical operations
- is.na() is used to test for NA
- is.nan() is used to test for NaN
- NA values have a class also (integer NA, character NA, etc)
- A NaN value is also NA

```{r}
# create a vector with NAs in it
x <- c(1,2,NA,10,NaN,3)
# Return logical vector indicating which elements are NA
is.na(x)
# Return logical vector indicating which elements are NaN
is.nan(x)
```

## Data Frames
Used to store tabular data in R. Important in many statistical modeling. dplyr has optimized set of functions to work with data frames.  
Represented as special type of list where everyelement of the list has to have the same length. Each element can be thought of as a column and the length of each element of the list is the number of rows.  
Unlike matrices, can store different classes of objects in ach column.  
In addition to column names, DF have attribute called row.names to indicate informration about each row.  
Usually created by reading a dataset using read.table() or read.csv(). Can be created with data.frame() or converted from other objects.  
Can be converted to matrix with data.matrix(). 
```{r}
x <- data.frame(foo = 1:4, bar = c(TRUE, TRUE, FALSE, FALSE))
x
nrow(x)
ncol(x)
```

## Names
R objects can have names, helpful for readable code. 
```{r}
x <- 1:3
names(x)
names(x) <- c("New York", "Seattle", "Los Angeles")
x
names(x)

# Lists can also have names
x <- list("Los Angeles" = 1, Boston = 2, London = 3)
x
names(x)

# Matrices can have both row and column names
m <- matrix(1:4, nrow = 2, ncol = 2)
dimnames(m) <- list(c("a","b"),c("c","d"))
m
# Column names and row names can be specified separately
colnames(m) <- c("h","f")
rownames(m) <- c("x","z")
m
```
Note that for dataframes, there is a separate function for setting row names, the row.names(). DF do not have column names, just names. So you use the names() function. 

# Getting Data In and Out of R
## Reading and Writing Data
There are a few principal functions
- read.table, read.csv for reading tabluar data
- readLines for reading lines of a text file
- source, for reading in R code files (inverse of dump)
- dget, for reading in R code files (inverse of dput)
- load, for reading in saved workspaces
- unserialize, for reading simple R objects in binary form. 

Many packages exist for other datasets

Analogous functions for writing to files
- write.table for writing tabular to text files or connections
- writeLines, for writing character data line-by-line to file or conn
- dump, for dumping a textual representation of multiple R objects
- dput, for outputting a textual representation of an R object
- save, for saving an arbitrary number of R objects in binary format to a file
- serialize, for converting an R object into a binary format for outputting to a connection. 

## Reading Data Files with read.table()
Commonly used. Help file is worth reading. 
Arguments: 
- file, name of file, or connection
- header, logical indicating if file has a header
- sep, a string indicating how columns are separated
- colClasses, character vector indicating the class of each column
- nrows, number of rows. Default reads whole file
- comment.char, chacter string indicating comment character. 
- skip, number of lines to skip from beginning
- stringsAsFactors, should character variables be coded as factors? defaults to True. 

For small to moderate size, you can usually call read.table
```{r}
#data <- read.table("foo.txt")
```
- In this case, R will automatically skip lines that begin with #
- figure out how many rows there are and how much mem is needed. 
- figure out type of variable in each column. 

Telling R these things directly makes R faster and more efficient. 

## Reading in larger data sets with read.table
With larger data sets, make life easier and prevent R from choking by: 
- Read the help page for read.table
- Make a rough calculation of the memory required to store your dataset. If dataset is larger than RAM, stop. 
- set comment.char = "" if no commented lines in file
- Use colClasses argument. Specifing this can make it run **much** faster, often twice as fast. 

```{r}
#initial <- read.table("datatable.txt",nrows=100)
#classes <- sapply(initial,class)
#tabAll <- read.table("datatable.txt",colClasses=classes)
```
- set nrows. Doesn't make faster but helps with memory. 

Also useful to know about your system. 
- How much mem is available? 
- Can you close other applications? 
- Are there other users logged in? 
- What OS are you using? Some limit amount of memory. 

## Calculating Memory Requirements for R Objects
Because R stores all objects in physical memory, important to keep limits in mind. Especially when reading in a new dataset to R. BOTE are easy for it.  
Suppose data frame with 1,500,000 rows and 120 columns all of which are numeric. How much memory to store numeric data? 
1,500,000 x 120 x 8 bytes/numeric
= 1,440,000,000 bytes 
= 1,400,000,000 / 2^20 bytes/MB
= 1,373.29 MB
= 1.34 GB.

# Using the readr Package
Recently developed to deal with reading large flat files quickly, replacement for read.table() and read.csv(). Ananogous in readr are read_table() and read_csv().  
For the most part, can use these instead. 
```{r}
library(readr)
#teams <- read_csv("data/team_standings.csv")
#teams
```
By default, will open and read lint-by-line. Will also, read first few rows of table to figure out type of column. Can instead specify type of each column with col_types argument. Good idea in general.  
col_types accepts a compact representation. "cc" says first and second columns are characters.  
read_csv also reads compressed files automatically, no need to decompress.  
Can specify column type in detailed fashion using various col_* functions. 

# Using Textual and Binary Formats for Storing Data
Data can be stored in a variety of ways including strutured forms like CSV. Intermediate format that is textual, but not as simple as CSV.  
One can create a more descriptive representation of an R object by using the dput() or dump() functions; useful because resulting textual format is editable and recoverable in case of corruption. Preserve metadata (sacrificing readability) so user doesn't have to specify it again.  
Work much better with VC programs to track meaningful changes.  
Downsides include not space inefficiency and partial readibility. 

## Using dput() and dump()
One way to pass data around is deparsing the R object with dput() and reading it back in with dget().  
```{r}
# Create a data frame
y <- data.frame(a = 1, b = "a")
# Print 'dput' output to console
dput(y)
```
Notice that the dput() output is in the form of R code and that it preserves meta data like class, row names, and column names.  
Output can also be saved directly to a file
```{r}
# Send 'dput' output to a file
dput(y, file = "y.R")
# Read in 'dput' output from a file
new.y <-dget("y.R")
new.y
```
Multiple objects can be deparsed at once using the dump function and read back in using source
```{r}
x <- "foo"
y <- data.frame(a = 1L, b = "a")
dump(c("x","y"),file = "data.R")
rm(x,y)
# Inverse of dump is source
source("data.R")
str(y)
x
```
# Binary Formats
The complement to the textual format is the binary format. Use for efficiency purposes or if no useful way to represent data with text. Also, can lose precision on numeric data when converting to and from textual format.  
Convert R object to binary: save(), save.image(), serialize().  
Individual objects with save() function
```{r}
a <- data.frame(x = rnorm(100), y = runif(100))
b <- c(3, 4.4, 1/3)
# Save 'a' and 'b' to a file
save(a, b, file = "mydata.rda")
# Load 'a' and 'b' into workspace
load("mydata.rda")
```
Save lots of objects with save.image()
```{r}
# Save everything
save.image(file = "mydata.RData")
# Load all objects in this file
load("mydata.RData")
```
Example used .rda for save and .RData for save.image. Can use other formats but these are common and accessible.  
serialize() converts individual R objects to binary format that can be communicated across an arbitrary connection (file or network).  
Coded as a raw vector in hexadecimal. 
```{r}
x <- list(1, 2, 3)
serialize(x,NULL)
```
Can be sent to a file, but better of using save for that.  
Benefit to serialize: perfectly represent R object in an exportable format, without losing precision or metadata. 

# Interfaces to the Outside World
Data are read in using *connection* interfaces. Connections can be made to files or more exotic things. 
- file, opens connection to file
- gzfile, opens connection to file compressed with gzip
- bzfile, opens connection to file compressed with bzip2
- url, opens connection to webpage. 

Think of connections as translator that lets you talk to objects outside of R.  
## File Connections
Connections to file using file()  
```{r}
str(file)
```
Arguments: 

- description, name of file
- open, mode to open file 
  + 'r' read mode
  + 'w' write mote (and initialize new file)
  + 'a' appending
  + 'rb', 'wb', 'ab' readin writing appending in binary

Most of connection is dealt with in background and we don't deal with it. 
```{r, eval = FALSE}
# Create connection to 'foo.txt'
con <- file("foo.txt")
# Open connetion in read mode
open(con, "r")
# Read from connection 
data <- read.csv(con)
# Close connection
close(con)

# this is all the same as: 
data <- read.csv("foo.txt")
```
In the background, read.csv() opens a connection, reads, and closes the connection. 

# Reading Lines of a Text File
Lines of text file can be read line by line with readLines(). Useful for unstructured and nonstandard data.  
```{r}
# Open connection to gz-compressed file
# con <- gzfile("words.gz")
# x <- readLines(con,10)
# x
```
For more structured text like CSV, other functions like read.csv() and read.table() exits.  
In above, gzfile() is used to create connections to compressed data with gzip algorithm. Lets you not unzip files.  
Complementary function writeLines() to write character vector element wise to each line. 

## Reading from a URL Connection
readLines() is useful for reading in lines of webpages that are text files stored on a server. url() function navigates communication between computer and web server. 
```{r}
# Open a url connection for reading
con <- url("https://www.jhu.edu","r")

# Read the webpage
x <- readLines(con)

# Print first few lines
head(x)
```
Reading a simple page is sometimes useful, more commonly used to read data stored on web servers.  
Using URL connections can be useful for producing a reproducible analysis, because code documents where the data cam from and how it was obtained. Preferable to opening a webpage and downloading dataset by hand. If server is changed, can break code. 

# Subsetting R Objects
Three operators to extract subsets of R Objects

* The "[]" operator always returns an object of the same class as the original, can be used to select multiple elements of object
* [[]] operator is used to extract elements of a list or data frame. Used to extract single element and class of returned object will not necessarily be a list or data frame. 
* $ operator is used to extract elements of a list by literal name. 

## Subsetting a Vector
Vectors are basic objects and are subsetted using the [] operator.  
```{r}
x <- c("a", "b", "c", "c", "d", "a")
x[1]
x[2]
# Can be used to extract multiple elements by passing an inteer sequence. 
x[1:4]
# Sequence doesn't need to be in order
x[c(1, 3, 4)]
# Can also pass logical sequence. For example, elements of x that come alphabetically after letter a
u <- x > "a"
x[u]
# More compactly:
x[x > "a"]
```

# Subsetting a Matrix
Subsetted in usual way with (*i*, *j*) indices. Row x column
```{r}
x <- matrix(1:6, 2, 3)
x
# access elements using indices
x[1,2]
x[2,1]
# Missing index indicates entire row/column
x[1,]
x[,2]
```
### Dropping Matrix Dimensions
By default, when you index a single element a vector length 1 is returned. Can turn this off. 
```[r}
x <- matrix(1:6, 2, 3)
x[1,2]
x[1, 2, drop = FALSE]
# Similar use to keep matrix dimensions for extracting entire row. 
x[1,]
x[1, , drop = FALSE]
```
**Be careful of R's automatic dropping of dimensions**

## Subsetting Lists
```{r}
x <- list(foo = 1:4, bar = 0.6)
x
## The [[]] operator can be used to extract a single element from a list
x[[1]]
## Can also use named indices or $ operator to extract by name
x[["bar"]]
x$bar
# Don't need quotes on $ operator. 
# [[ can be used with computed indices, $ operator can only be used with names
x <- list(foo = 1:4, bar = 0.6, baz = "hello")
name <- "foo"

# Computed index for "foo"
x[[name]]
# element name doesn't exist! 
x$name
```

## Subsetting Nested Elements of a List
The [[]] operator can take an integer sequence
```{r}
x <- list(a = list(10, 12, 14), b = c(3.14, 2.81))
# Get 3rd element of the first element
x[[c(1,3)]]
# Alternative
x[[1]][[3]]
# first element of second element
x[[c(2,1)]]
```

## Extracting Multiple Elements of a List
The [] operator can extract multiples
```{r}
x <- list(foo = 1:4, bar = 0.6, baz = "hello")
x[c(1,3)]
```
Note that x[c(1,3)] is not the same as x[[c(1,3)]]! 

# Dates and Times
R has a special representation for date and time with. Date class for dates and POSIXct or POSIXlt for times. Date is stored as number of days since 1970-01-01 while time is number osf seconds since 1970-01-01. 

## Dates in R
Dates are represented by Date class and can be coerced from a character string with as.Date(). 
```{r}
# Coerce a date object from character
x <- as.Date("1970-01-01")
x
# You can see internal representation of Date object by using the unclass() function. 
unclass(x)
unclass(as.Date("1970-01-02"))
```

## Times in R
Times are represented by POSIXct or POSIXlt. 
POSIXct is just a large integer underneath and useful for storing in data frame. 
POSIXlt is a list underneath and stores extra info like day of week, year, month, day of month. Useful if you need extra info. 

Generic Functions

- weekdays: gives day of week
- months: gives the month dame
- quarters: gives the quarter number "Q1" etc. 

Times can be coerced from character string using as.POSIXlt or as.POSIXct
```{r}
x <- Sys.time()
x
class(x)
# POSIXlt has useful metadata
p <- as.POSIXlt(x)
names(unclass(p))
p$wday

# can also use POSIXct format
x <- Sys.time() 
x # already in POSIXct format
unclass(x) # Internal representation
# x$sec #Won't work! 
p <- as.POSIXlt(x)
p$sec

# strptime() for dates in different format. Takes character vector and converts them to POSIXlt object. 
datestring <- c("January 1, 2012 10:40", "December 9, 2011 9:10")
x <- strptime(datestring, "%B %d, %Y %H:%M")
x
class(x)
```
The % symbols are for formatting strings for dates and times. Probably not worth memorizing. Check ?strptime for details. 

## Operations on Dates and Times
Can use +, - and comparisions (==, <=, etc)

```{r}
x <- as.Date("2012-01-01")
y <- strptime("9 Jan 2011 11:34:21", "%d %b %Y %H:%M:%S")
# x - y doesn't work! 
x <- as.POSIXlt(x)
x - y
```
A nice feature of this class is it keeps track of leap years, daylight savings, time zones. 
```{r}
x <- as.Date("2012-03-01")
y <- as.Date("2012-02-28")
x - y
# Timezone
x <- as.POSIXct("2012-10-25 01:00:00")
y <- as.POSIXct("2012-10-25 06:00:00", tz="GMT")
y - x
```

# Managing Data Frames with the dplyr package
## Data Frames
DF are a key structure. Basic structure is one observation per row and each column represents a variable, measure, feature, or characteristic. Extra formats downloadable from CRAN that give better implementation for things like large datasets. 
Tools for dealing with them include subset(), [], and $ to extract subsets. Other operations with base R like filtering, reordering, and collapsing can be tedious. dplyr helps mitigate and optimize these challenges. 

## The dplyr Package
Doesn't introduce anything new, but it *greatly* simplifies existing functionality. It provides a "grammar" for data manipulation and operating on data frames. Helps communicate what you are doing to a data frame. Also, dplyr is very fast. 

## dplyr Grammar
key "verbs":

* select: return a subset of the columns of a data frame, using flexible notation
* filter: extract a subset of rows from data frame based on logical conditions
* arrange: reorder rows of a data frame
* rename: rename variables in data frame
* mutate: add new variables/columns or transform existing variables. 
* summarize/summarise: generate summary table of statistics of different variables in data frame, possibly within strata. 
* %>%: the "pipe" operator is used to connect multiple verb actions into a pipeline. 

dplyr package also includes a number of data types. 

### Common dplyr function properties

1. The first argument is a data frame
2. The subsequent arguments describe what to do with the data frame, and you can refer to columns in the data frame directly without the $ operator. 
3. The return result of the function is a new data frame. 
4. Data frames must be properly formatted and annotated to be useful. Data should be tidy: one observation per row, each column with a feature or characteristic of observation. 

## Installing dplyr package
Can be installed from CRAN or GitHub uisng devtools pacakage and install_github() fuction. GitHub usually has latest updates of function.  
To install from CRAN, run
```{r, eval = FALSE}
install.packages("dplyr")
```
To install from GitHub use
```{r, eval = FALSE}
install_github("hadley/dplyr")
```
Remember to load into R session with library()!
```{r, eval = FALSE}
library(dplyr)
```
If R gives warnings about functions with the same name, ignore for now.

## select()
Examples in this section will use dataset available from textbook's website. Download an unzip then load using readRDS()
```{r, eval = FALSE}
chicago <- readRDS("chicago.rds")
# Basic characteristics
dim(chicago)
str(chicago)
```
select() function to select columns to focus on. You'll often have large dataset but any given analysis will use subset. 
Suppose we want first 3 columns. Can use numeric index or column names directly. 
```{r, eval = FALSE}
names(chicago)[1:3]
subset <- select(chicago,city:dptp)
head(subset)
# Note that the : normally cannot be used with names of strings, but inside select() can specify range of var names. 
# Can also omit variables using negative sign. 
select(chicago, -(city:dptp))
# Indicates every variable except city through dptp. Equivalent code:
i <- match("city",names(chicago))
j <- match("dptp", names(chicago))
head(chicago[,-(i:j)]
```
Select also lets special syntax for patterns
```{r, eval = FALSE}
# keep var ending with "2"
subset <- select(chicago,ends_with("2"))
str(subset)
# or keeping every variable that starts with "d"
subset <- select(chicago, starts_with("d))
str(subset)
```
You can also use general regex. 

## filter()
Extract subset rows. Similar to subset() but faster. 
Extract rows with PM2.5 greater than 30
```{r, eval = FALSE}
chic.f <- filter(chicago,pm25tmean2 > 30)
str(chic.f)
summary(chic.f$pm25tmean2)

# Can extract arbitrarily complex logical sequence inside of filter()

chic.f <- filter(chicago, pm25tmean > 30 & tmpd > 80)
select(chic.f, date, tmpd, pm25tmean2)
```

## arrange()
Used to reorder rows according to one of the variables/columns. Reordering, while preserving order of other columns, is a pain in R. arrange() simplifies the process. 
```{r, eval = FALSE}
chicago <- arrange(chicago, date) #starts at earliest, ends at oldest
head(select(chicago, date, pm25tmean2), 3)
tail(select(chicago, date, pm25tmean2), 3)

# Columns can be arranged in descending order too using desc() operator
chicago <- arrange(chicago, desc(date))
head(select(chicago, date, pm25tmean2),3)
tail(select(chicago, date, pm25tmean2),3)
```

## rename()
Renaming a variable in R is surprisingly hard to do! rename() makes it easier. 
```{r, eval = FALSE}
head(chicago[, 1:5],3) # names of first 5 variables
# dptp represents dew point temp
# pm25tmean2 provides PM2.5 data
# The given names are obscure and can be renamed
chicago <- rename(chicago, dewpoint = dptp, pm25 = pm25tmean2)
head(chicago[,1:5],3)
```

## mutate()
Compute transformations of variables, such as deriving new variables from existing. 
For example, can 'detrend' data by subtracting a mean to see how it differs from average rather than absolute level. 
```{r, eval = FALSE}
chicago <- mutate(chicaago, pm25detrend = pm25 - mean(pm25, na.rm = TRUE))
head(chicago)
```
transmute() does the same as mutate but then *drops all non-transformed variables*
```{r, eval = FALSE}
head(transmute(chicago,
  pm10detrend = pm10tmean2 - mean(pm10tmean2, na.rm = TRUE),
  o3detrend = o3tmean2 - mean(o3tmean2, na.rm = TRUE)))
```

## group_by()
Creates summary statistics from the data frame within strata defined by a variable. For example, can compute average annual level of PM2.5. Stratum is the year, derived from date. 
Standard operation is split data frame into separate pieces defined by variable or group of variables with group_by(), then apply summary function.
```{r, eval = FALSE}
chicago <- mutate(chicago, year = as.POSIXlt(date)$year + 1900) # year variable
years <- group_by(chicago, year) # create separate data frame that splits original DF by year. 
# Finally, compute summary statistics
summarize(years), pm25 = mean(pm25, na.rm = TRUE),
  o3 = max(o3tmean2,na.rm = TRUE),
  no2 = median(no2tmean2,na.rm = TRUE),
  .groups = "drop")
 
```
summarize() returns a dataframe with year as first column, then annual averages of pm25, o2, and no2. 
As a more complicated example, might want to know average levels of ozone and nitrogen dioxide within quintiles of pm25. A regression model would be slicker, but we can do quickly with group_by() and summarize(). 

```{r, eval = FALSE}
qq <- quantile(chicago$pm25,seq(0,1,0.2),na.rm = TRUE)
chicago <- utate(chicago, pm25.quint = cut(pm25,qq))
# Now we can group the data frame by the pm.25 quint variable. 
quint <- group_by(chicago, pm25.quint)
# Finally, compute mean within quintiles. 
summarize(quint, o3 = mean(o3tmean2, na.rm = TRUE),
  no2 = mean(no2tmean2, na.rm = TRUE),
  .groups = "drop")
```

From the table above, there isn't a strong relationship between pm25 and o3, but a positive correlation between pm25 and no2. More sophisticated stat modeling can give precise answers to these questions, but this application can get you most of the way there. 

## %>%
The pipeline operator is handy for stringing together multiple functions in a sequence of operations. Above, we had to nest the functions when applying multiple. The pipeline operator lets you string operations left to right instead of nesting. In the last section we had to: 

1. create a new variable pm25.quint
2. split the data frame by that new variable
3. compute the mean of o3 and no2 in the sub-groups defined by pm25.quint

That can be done with the following sequene: 
```{r, eval = FALSE}
mutate(chicago, pm25.quint = cut(pm25,qq)) %>%
  group_by(pm25.quint) %>%
  summarize(o3 = mean(o3tmean2, na.rm = TRUE),
    no2 = mean(no2tmean2, na.rm = TRUE),
    .groups = "drop")
```
This helps us not create a set of temporary variables along the way or massive nested sequences. The first argument of a pipeline is taken to be the output of the previous element in the pipeline. 
Another example is average pollutant level by month. 
```{r, eval = FALSE}
mutate(chicago, month = as.POSIXlt(date)$mon + 1) %>%
  group_by(month) %>%
  summarize(pm25 = mean(pm25, na.rm = TRUE),
    o3 = max(o3tmean2, na.rm = TRUE),
    no2 = median(no2tmean2, na.rm = TRUE),
    .groups = "drop)
```

## Summary
the dplyr package provides a concise set of operations for managing data frames. 
Additonal benefits: 

* can work with other data frame "backends" such as SQL databases. There is an SQL interface for relational databases via the DBI package
* can be integrated with the data.table package for large fast tables.

# Control Structures
Control flow of execution, put "logic" into code. 
Commonly used strutures: 

* if and else: testing a condition and acting on it
* for: execute a loop a fixed number of times
* while: execute a loop while a condition is true
* repeat: execute an infinite loop (must break to stop)
* break: break execution of loop
* next: skip an interation

Not really used in interactive sessions, but when writing functions and long expressions. 

## if-else
Common conditional
Does nothing if condition is false, if you want an action when false, use else. 
Can have series of tests: 

## for loops
Takes an iterator variable and assign it successive values from a sequence or vector. Commonly used to iterate over elements of an object. 
```{r}
for(i in 1:10) {
  print(i)
}
```
This loop takes the variable i in each iteration of the loop and gives it values 1, 2, 3...10 executes the code, then exits. 
The following three loops have the same effect. 
```{r}
x <- c("a", "b", "c", "d")
for(i in 1:4) {
  print(x[i])
}
# seq_along() is commonly used in conjunction with for loops to generate
# integer sequence based on length of object. 
for(i in seq_along(x)) {
  print(x[i])
}
# Not necessary to use an index-type variable. 
for(letter in x) {
  print(letter)
}
# For one line loops, can skip braces
for(i in 1:4) print(x[i])
```
Not a bad idea to always use curley braces - makes it easier to expand function at later date. 

## Nested for loops
```{r}
x <- matrix(1:6, 2, 3)

for(i in seq_len(nrow(x))) {
  for(j in seq_len(ncol(x))){
    print(x[i,j])
  }
}
```
Commonly used for multidimensional or hierarcharal data structures. Avoid more than 2-3 levels of nesting as it can make code difficult to read. Consider breaking up loops with functions. 

## while loops
Begin by testing a condition, if true body executes. 
```{r}
count <- 0
while(count < 10) {
  print(count)
  count <- count + 1
}
```
Can create infinite loops if not careful! 
Can have multiple conditions, evaluated left to right. 
```{r}
z <- 5
set.seed(1)
while(z >= 3 && z <=10) {
  coin <- rbinom(1,1,0.5)
  if(coin == 1) { # random walk
      z <- z + 1
    } else {
      z <- z - 1
    }
}
print(z)
```

## repeat loops
Creates an infinite loop, not commonly used in stats applications. Can only be exited with a break
Possible paradigm might be iterative algorithm where you are searching for a solution and don't want to stop till you're close enough. Often don't know how many iterations itll take to get close enough. 
```{r, eval = FALSE}
x0 <- 1
tol <- 1e-8

repeat {
  x1 <- computeEstimate()
  
  if(abs(x1 - x0) < tol) { #close enough? 
    break
  } else {
    x0 <- x1
  }
}
```
Above code won't run if the computeEstimate() function is not defined. 
Looping this way is dangerous as no guarantee itll stop. Can set hard limit on number of iterations using a for loop and then report on convergence. 

## next, break
next skips an iteration
```[r}
for(i in 1:100) {
  if(i <=20) {
    next
  }
  # do something else
}
# break exits immediately
for(i in 1:100) {
  print(i)
  if(i > 20) {
    break
  }
}
```
# Functions
Core activity of R programmer. Step from user to developing new functionality for R. Encapsulate sequence of expressions that need to be excuted numerous times, perhaps under different conditions. Often used when code is shared with others or public.  
Allows develper to create interfact to code, explicitly specified with set of parameters. Provides abstraction of code to user to simplify process so user doesn't have to know every detail of code. Can allow developer to communicate to user aspects of code that are important or relevant. 

## Functions in R
'first class objects' treated like any other R object. 

* Functions can be passed as arguments to other functions. 
* Can be nested. 

## Your first function
Defined using function() directive and stored as R objects of class function. 
```{r}
f <- function() {
  # This is an empty function
}
class(f)
f()
```
Nontrivial example:
```{r}
f <- function() {
  cat("Hello, world! \n")
}
f()
```
Can include arguments that user may explicitly set. 
```{r}
f <- function(num) {
  for(i in seq_len(num)) {
    cat("Hello, world!\n")
  }
}
f(3)
```
Can also return objects
```{r}
f <- function(num) {
  hello <- "Hello, world!\n"
  for(i in seq_len(num)) {
    cat(hello)
  }
  chars <- nchar(hello) * num
  chars
}
meaningoflife <- f(3)
print(meaningoflife)
```
Return value of a function is always the very last expression that is evaluated. 
There is a return() function to explictly return values, rarely used in R. 
If num wasn't specified above, R will give an error. Can set default. 
formals() will return list of all the formal arguments of a function. Functions have named arguments that can have default values so you can specify argument by name. Useful if has lots of arguments. 

## Argument matching
Arguments of functions can be matched by position or by name. 
```{r}
str(rnorm)
mydata <- rnorm(100, 2, 1)
```
When specifying arguments by name, order doesn't matter. 
You can mix positional and name matching. Once an argument is matched by name, it is 'taken out' of the argument list and the remaining unnamed arguments are matched in the listed order. 
```{r, eval = FALSE}
args(lm)
# Following two calls are equivalent
lm(data = mydata, y~x, model = FALSE, 1:100)
lm(y~x, mydata, 1:100, model = FALSE)
```
Arguments can also be partially matched. 

1. Check for exact match for a named argument
2. Check for a partial match
3. Check for a positional match

Partial matching should be avoided when writing long code or programs as it can lead to confusion. Useful for interactive use. 

## Lazy Evaluation
Arguments to functions are evaluated lazily, so only evaluated as needed in body of function. 
For example: 
```{r}
f <- function(a,b) {
  a^2
}
f(2)
```
This function never uses argument b, so calling f(2) will not produce an error because 2 is positionally matched to a. 

## The ... argument
Indicates a variable number of arguments that are usually passed onto other functions. Often used when extending another function and you don't want to copy the entire argument list of the original function. 
Example: custom plotting
```{r}
myplot <- function(x,y,type='l',...) {
  plot(x,y,type = type,...) # passes '...' to 'plot' function
}
```
  Generic functions use ... so that extra arguments can be passed to methods. 
```{r}
  mean
```
The ... argument is necessary when the number of arguments passed to the function cannot be known in advance. Clear functions like paste() and cat(). 

## Arguments after the ... argument
One catch - arguments after ... must be named explicitly and cannot be matched partially or positionally. 
```{r}
args(paste)
```
With the paste() function, the arguments sep and collapse must be named explicitly if default values are not used. 

# Scoping Rules of R
## A diversion on Binding Values to Symbol 
How does R know which value to assign to which symbol?
```{r}
lm <- function(x) {x * x}
lm
```
How does R know what value to assign to lm? Why doesn't it give it the value of lm that is in the stats package? 
When R tries to bind a value to a symbol, it searches through a series of environments to find the appropriate value. When you work on the command line, R follows (roughly) the order: 

1. Search global environment (workspace) for a symbol name matching the one requested. 
2. Search the naamespaces of each package on the search list. 

The search list can be found with search()
```{r}
search()
```
The *global environment*, or user's workspace, is always the first and base is always the last. The order matters, particularly if there are multiple objets with the same name in different packages.  
Users load packages at startup, so if you are writing a function or package  you cannot assume a default order. When a user loads a package with library(), the namespace of that package is at position 2 and eerything else is shifted.  
Note R has separate namespaces for functions and non-functions so you can have an object named c and a function named c().  

## Scoping Rules
Main feature that makes R different from S.  
The scoping rules of a language determine how a value is associated with a *free variable* in a function. R uses *lexical scoping* or *static scoping*. An alternative is *dynamic scoping*. Lexical scoping is useful for simplifying statistical computations.  
Related to scoping rules is how R uses the search list to bind a value to a symbol. Consider: 
```{r, eval = FALSE}
f <- function(x,y) {
x^2 + y / z
}
```
This function has 2 formal arguments x and y. In the body is z, a free variable. Free variables are not formal arguments and are not local variables (assigned inside a function's body). In lexical scoping, the values of free variables are searched for in the environment in which the function was defined. 
What is that environment? An *environment* is a cllection of (symbol, value) pairs, ie x is the symbol and 3.14 is the value. Every environment has a parent environment and maybe multiple children. Only environment without a parent is the empty environment.  
A function, together with an environment, make up a *closure* or *function closure*. Most of the time, we don't need to think about a function and its associated environment, but can be useful to consider. Can be used to create functions that "carry around" data with them.  
How do we associate a value to a free variable? Search process: 

* If the value of a symbol is not found in the environment in which a function was defined, then continue to parent environment. 
* Continue down sequence of parent environments till we hit the *top-level envornment*, usually the global environment or namespace of a package. 
* After the top-level environment, continue down search list till we hit the empty environment. 

If a value for a symbol isn't found by end, then an error is thrown.  
One consequence: it can be affected by the number of packages you have attached to search list. More packages -> more symbols to search. Would need a LOT of packages attached to  notice a real difference in speed. 

** Lexical scoping: why does it matter? 
Typically, a function is defined in the global environment, so that the values of free variables are found in the user's workspace. his behavior is logical and usually the 'right thing'. However, if a function is defined inside another function, the environment in which a function is defined is the body of another function. 
```{r}
make.power <- function(n) {
  pow <- function(x) {
    x^n
  }
  pow
}
```
The make.power() function is a kind of 'constructor function' that can be used to construct other functions. 
```{r}
cube <- make.power(3)
square <- make.power(2)
cube(3)
square(3)
```
If we look at cube()'s code, we see a free variable n. WHat is the value of n? It is taken from the environment where the function was defined. Cube was defined when make.power(3) was called, so the value of n was 3.  
We can explore the environment of a function. 
```{r}
ls(environment(cube))
get("n",environment(cube))
```

## Lexical vs Dynamic Scoping
Example: 
```{r}
y <- 10
f <- function(x) {
  y <- 2
  y^2 + g(x)
}
g <- function(x) {
  x*y
}
```
What is the value of f(3)? 
With lexical scoping, the value of y in the function g is looked up in the environment in which the function was defined, in this case the global environment, so the value of y is 10. With dynamic scoping, the value of y is looked up in the environment from which the function was called (sometimes referred to as the calling environment). In R the calling environment is known as the parent frame. In this case, the value of y would be 2. 
When a function is defined in the global environment and subsequently called from the global environment, then the defining environment are the same. This can give the appearance as dynamic scoping. 
Lexical scoping in R has consequences beyond how free variables are looked up. Particularly, its why all objects must be stored in memory: all functions must carry a pointer to their respective defining environments, which could be anywhere. 

## Application: Optimization
Why is this info on lexical scoping useful? 
Optimization routines in R like optim(), nlm(), and optimize() require you to pass a function whose argument is a vector of parameters (eg log-liklihood, or cost function). However, an objective function that needs to be optimized might depend on a host of other things (like data). WHen writing optimizers, it might be desirable to allow user to hold certain parameters fixed. Scoping rules of R allow you abstract away much of the complexity involved in these kinds of problems. 
Example of a constructor that creates a negative log-liklihood function that can be minimized to find maximum liklihood estimates in a statistical model. 
```{r, eval = FALSE}
make.NegLogLik <- function(data, fixed = c(False, False)) {
  params <- fixed
  function(p) {
    params[!fixed] <- p
    mu <- params[1]
    sigma <- params[2]
    
    # Calculate the Normal density
    a <- -0.5*length(data)*log(2*pi*sigma^2)
    b <- -0.5*sum((data-mu)^2) / (sigma^2)
    -(a+b)
  }
}
```
Note: optimization functions in R minimize functions, so you need to use the negative log-liklihood. 
```{r}
set.seed(1)
normals <- rnorm(100,1,2)
nLL <- make.NegLogLik(normals)
nLL
## What's in the function environment? 
ls(environment(nLL))
## Now that we have nLL() function, can try to minimize it with optim()
## to estimate parameters. 
optim(c(mu = 0, sigma = 1),nLL)$par
```
Can also try to estimate one parameter while holding another parameter fixed. 
```{r}
nLL <- make.NegLogLik(normals,c(FALSE,2))
optimize(nLL,c(-1,3))$minimum
```
Because of 1D problem, can use simpler optimize() function rather than optim(). 

## Plotting the Likelihood
Another nice feature you can use is plotting the negative log-liklihood to see how peaked or flat it is. 
Check out text for examples. 

# Coding Standards for R
By no means universal. 
**Always use text files/text editor** Use RStudio or similar, not Word  
**Indent your code** Greatly helps readability. Min 4 spaces, 8 ideal.  
**Limit the width of code** Limit width of text editor so it doesn't fly off right hand side. Deliberately limits ability to write very long functions with many levels of nesting.  
**Limit length of individual functions** Ideal purpose of function is to execute one activity or idea. Rule of thumb: no more than one page of editor. 

#Skipped Section 17

# Loop Functions
## Looping on the Command Line
Writing for and while loops is useful when programming but not as easy on command line. Compact looping functions: 

* lapply(): loop over a list and evaluate a function on each element
* sapply(): same as lapply but try to simplify result
* apply(): apply a function over the margins of an array
* tapply(): apply a function over subsets of a vector
* mapply(): multivariate version of lapply

Auxiliary function split is also useful, particularly in conjunction with lapply. 

## lapply()
Does the following: 

1. Loops over list, iterating over each element of list
2. applys a function to each element of list
3. returns a list

Takes 3 arguments: 1, a list, 2, a function and 3, other arguments via .... If x is not a list, it will be coerced to one using as.list().  
```{r}
lapply
```
Note looping is done internally in C for efficiency. 
Note lapply() returns a list.  
Example of applying mean to a list. 
```{r}
x <- list(a = 1:5, b = rnorm(10))
lapply(x,mean)
```
Passing the mean function as an argument to lapply will apply mean to each element of the list. Note you don't need () on the function.  
Can use lapply to evaluate a function multiple times with each different argument. runif() generates uniform distributed random variables according to number input.  
```{r}
x <- 1:4
lapply(x,runif)
```
When you pass a function to lapply, it takes elements of the list and passes them as the *first argument* of the function you are applying. In above, first argument of runif is n, so elements of 1:4 got passed to n argument of runif.  
Can have other arguments for function. Fill them with the ... argument of lapply.  
```{r}
x <- 1:4
lapply(x,runif, min = 0, max = 10)
```
This passes min = 0 and max = 10 to runif every time.  
lapply and company use a lot of anonymous functions that are generated on the fly. 
```{r}
x <- list(a = matrix(1:4, 2, 2), b = matrix(1:6, 3, 2))
# Suppose we want to extract first column of each matrix in a list. 
lapply(x, function(elt) {elt[,1]})
```
Function definition inside lapply! Use anonymous functions for one off, if going to use repeatedly, better to define it separately. 

## sapply()
Similar to lapply, but tries to simplify output. 

* If result is list with every element is length 1, returns a vector
* if result is a list with every element a vector of same length, returns matrix
* If can't parse, a list is returned. 

## split()
Takes a vector or other object and splits it into groups determined by a factor or list of factor.  
Arguments: 
```{r}
str(split)
```
where: 

* x is a vector (or list) or data frame
* f is a factor (or coerced to one) or a list of factors
* drop indictates whether empty factors levels should be dropped

Combination of split and lapply, sapply is common in R. Paradigm is you take a data structure, split it into subsets defined by another variable, and apply a function over the subsets. Result is collated and returned as an object. Sometimes referred to "map-reduce". 
```{r}
x <- c(rnorm(10), runif(10), rnorm(10,1))
f <- gl(3,10) # generate levels in a factor variable
split(x,f)
# commonly see split followed by lapply. 
lapply(split(x,f),mean)
```
## Splitting a data frame
```[r}
library(datasets)
head(airquality)
# split by Month variable
s <- split(airquality, airquality$Month)
str(s)
# Can then take column means for Ozone, Solar.R and Wind for each sub-data frame. 
lapply(s,function(x) {
        colMeans(x[, c("Ozone","Solar.R", "Wind")]
        na.rm = TRUE)
})
# sapply may be better for more readable output
sapply(s,function(x) {
        colMeans(x[, c("Ozone","Solar.R","Wind")]
        na.rm = TRUE)
})
```
We may sometimes want to split an R object according to multiple variables. Can do this by creating an interaction with interaction(). 
```{r}
x <- rnorm(10)
f1 <- gl(2,5)
f2 <- gl(5,2)
f1
f2
# Create interaction
interaction(f1,f2)
# With multiple factors and many levels, creating an interaction may result in 
# many levels that are empty
str(split(x,list(f1,f2)))
# notice 4 empty categories, can drop empty when calling split. 
str(split(x,list(f1,f2), drop = TRUE))
```

# tapply
used to  apply function over subsets of vector. Essentially combination of split and sapply for vectors only. 
```{r}
str(tapply)
```
Arguments: 

* X is a vector
* INDEX is a factor or list of factors
* FUN is function to be applied
* ... other arguments to be passed to FUN
* simplify, should we simplify? 

Group means
```{r}
# simulate data
x <- c(rnorm(10), runif(10), rnorm(10,1))
# Define groups
f <- gl(3,10)
tapply(x,f,mean)

# Can also take group means without simplifying result, giving list. 
tapply(x,f,mean,simplify = FALSE)

# Can also use with functions that return more than one value
tapply(x,f,range)
```

## apply()
Evaluate a function (often anonymous) over margins of array, most often apply functon to rows or columns of a matrix (a 2d array). Can be used with general arrays, for example, to take the average of an array of matrices. Not really faster than a loop, but works in one line. 
```{r}
str(apply)
```

Arguments: 

* X is an array
* MARGIN is an integer vector indicating which margins should be 'retained'
* FUN is a function to be applied
* ... is for other arguments to be passed to FUN

```{r}
x <- matrix(rnorm(200),20,10) # 20 by 10 matrix of random normal numbers. 
apply(x,2,mean) #take mean of each column
apply(x,1,sum) # sum of each row
```
What is the margin argument? Whichever dimension of the array you want to retain. When taking the mean of columns, 2 specifies keeping columns and collapsing rows. WHen taking sum of rows, 1 specifies keeping rows and collapsing columns. 

## Col/Row Sums and Means
Special case of column/row sums and means of matrices: 

* rowSums = apply(x,1,sum)
* rowMeans = apply(x,1,mean)
* colSums = apply(x,2,sum)
* colMeans = apply(x,2,mean(

These shortcuts are optimized, faster, and descriptive. 

## Other ways to apply
Can compute other things such as quantile
```{r}
x <- matrix(rnorm(200),20,10)
# Get row quantiles
apply(x, 1, quantile, probs = c(0.25, 0.75))
```
Higher dimension example of averages of matrices
```{r}
a <- array(rnorm(2*2*10), c(2, 2, 10))
apply(a, c(1,2), mean)
```
Indicated via margin argument that first and second dimensions are preserved and third dimension is collapes via the mean. 

## mapply()
multivariate apply which applies a function in parallel over a set of arguments. Recall lapply and similar functions only iterate over a single object, mapply does multiple objects in parallel.  
```{r}
str(mapply)
```
Arguments: 

* FUN is a function to apply
* ... contains R objects to apply over
* MoreArgs is a list of other arguments to FUN
* SIMPLIFY indicates whether result should be simplified. 

Different argument order from lapply. 
Example: typing the following is tedious: 
`list(rep(1, 4), rep(2, 3), rep(3, 2), rep(4, 1))`
With mapply: 
```{r}
mapply(rep, 1:4, 4:1)
```
This passes the sequence 1:4 to the first argument of rep() and 4:1 to the second. 
Example for random normal variables
```{r}
noise <- function(n,mean,sd) {
        rnorm(n,mean,sd)
}
# Simulate 5 random numbers
noise(5, 1, 2)
# This only simulates 1 set of numbers, not 5
noise(1:5, 1:5, 2)
# Use mapply() to pass 1:5 separately to function so we get 5 sets
# of random numbers, each with different length and mean
mapply(noise(1:5, 1:5, 2)
# This is the same as
list(noise(1, 1, 2), noise(2, 2, 2), noise(3, 3, 2), 
        noise(4, 4, 2), noise(5, 5, 2))
```

## Vectorizing a Function
mapply() can be used to automatically vectorize a function, meaning it can be used to take a function that typically  only takes single arguments and create a new function that can take vector arguments. Often needed when you want to plot functions. 
Example of function that computes sum of squares
```{r}
sumsq <- function(mu, sigma, x) {
        sum(((x - mu) / sigma)^2)
}
```
This function takes a mean `mu`, a standard deviation `sigma`, and some data in a vector `x`.  
In many stats applications, we want to minimize sum of squares to find optimal `mu` and `sigma`. Before we do that, we may want to evaluate or plot the function for many different values of `mu` or `sigma`. However, passing a vector of `mu`'s and `sigma`s won't work with this function since its not vectorized. 
Use mapply
```{r}
mapply(sumsq,1:10, 1:10, MoreArgs = list(x = x))
```
Can even create a vectorized function with Vectorize(). 
```{r}
vsumsq <- Vectorize(sumsq, c("mu","sigma"))
vsumsq(1:10, 1:10, x)
```

# Debugging
## Something's Wrong! 
Can get following conditions: 

* message: a generic notification/diagnostic produced by message(); exeution of function continues. 
* warning: indication something is wrong, but not necessarily fatal; execution of function continues. produced by warning()
* error: indication of fatal problem and execution stops. Produced by stop() function. 
* condition: generic concept for indicating that something unexpected occurred, can create custom conditions. 

Example: calcuating log(-1) will give NaN. 
Lets you know that taking the log of a negative number results in a NaN value. Doesn't give error, because it does have a value it can return: NaN. Warning is just to let you know something happened.  
Error can pop if you don't take factors into account. 
For example: passing NA or NaN to an inequality will throw an error and cause the function to stop. If you suspect Na or NaN, should have if(is.na(x)) first and a way to handle it. 
Can get an error if incorrect input. Passing a vector to if will give an error since that function isn't vectorized. Can handle by creating a case to check length and stop the function. Can also create a new function with Vectorize().  

## Figuring out what's wrong 
Primary task is diagnosing what problem is. Important to first understand what is expected to occur, then identify what should have happened and how it deviated. Questions to ask: 

* What was your input, how did you call the funtion? 
* What were you expecting? output, messages, other results? 
* What did you get? 
* How does what you get differ from what you were expecting? 
* Were your expectations correct? 
* Can you reproduce the problem? 

## Debugging Tools in R

* traceback(): prints out the function call stack after an error occurs; does nothing if no error. 
* debug(): flags a function for "debug" mode which allows you to step through execution of a function one line at a time. 
* browser(): suspends the execution of a function whenever it is called and puts the function in debug mode
* trace(): allows you to insert debugging code into a function in specific places
* recover(): allows you to modify error behavior so you can browse the function call stack. 

## traceback()
Prints out the function call stack after error has occured. Function call stack is sequence of functions that was called before error occured. May have a() calling b() calling c() calling d(), finding where the error occured can be tricky without traceback(). 
```{r, error = TRUE}
mean(x)
traceback(x)
```
Clear that error is in mean() because x doesn't exist.  
traceback() must be called immediately after error occurs. 
```{r, error = TRUE}
lm(y ~ x)
traceback()
```
Shows error was not thrown till 7th level of function call stack when eval() realized y object does not exist. 
Useful to find roughly were error is, but not useful for detailed debugging. For that, turn to debug().  

## debug()
Makes an interactive debugger browser. Step through function one expression at a time to find where error occurs. 
```{r, error = TRUE}
debug(lm)
lm(y ~ x)
```
Now, every time lm() is called, it will launch debugger. Turn off with undebug(). 
Calls browswer on top level of function body. Then steps through eack expression. Special commands to call in browser: 

* `n` executes current expression and moves to next
* `c` continues with execution of function and doesn't stop till either error or function completes. 
* `Q` quits browser. 

While in browser, can execute any other R function available in regular session. In particular, `ls()` to see current environment and `print()` to print out values of R objects in function environment. 
